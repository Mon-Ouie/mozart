<CHAPTER ID="chapter.basics">
  <TITLE>Basic Operations</TITLE>

<SECTION><TITLE>Distribution model</TITLE>

<P> The distribution model gives well-defined distribution
behaviors to the Oz language entities.
The distribution behaviors are designed to do the
right thing by default in almost all cases.
For example, procedure code is transferred to sites
immediately, so that sites never need ask for
procedure code.
In the case of objects, the developer can
specify the desired distribution behavior, e.g.,
mobile (cached) objects, stationary objects,
stationary single-threaded objects.

<SUBSECTION><TITLE>Objects</TITLE>

<P> The most critical entities in a distributed application
in terms of network efficiency are the objects.
Objects have a state that has to be updated in a
globally-consistent way.
The efficiency of this operation depends
on the object's distributed behavior.
Many distribution behaviors are possible,
providing a range of trade-offs for the developer.
Here are some of the more useful ones:
<LIST>
<ITEM> <EM>Cached object</EM>:
Objects and cells are cached by default--we also call
this "mobile objects".
Objects are always executed locally,
in the thread that invokes the method.
This means that
a site attempting to execute a method will first fetch the object,
which requires up to three network messages.
After this, no further messages are needed as long
as the object stays on the site.
The object will not move as long as execution
stays within a method.
If many sites use the object, then it will travel
among the sites, giving everyone a fair share of the object use.

<P> The site where the object is created
is called its <EM>owner site</EM>.
A site requesting the object
first sends a message to the owner site.
The owner site then
sends a forwarding request to the site currently
hosting the object.
This hosting site then sends the object's
state pointer to the requesting site.

<P> The class of a cached object is copied to each
site that calls the object.
This is done lazily, i.e., the class is only
copied when the object is called for the first time.
Once the class is on the site, no further copies are done.

<ITEM> <EM>Stationary object (server)</EM>:
A stationary object remains on the site at which it was created.
Each method invocation uses one message to start the method
and one message to synchronize with the caller
when the method is finished.
Exceptions are raised in the caller's thread.
Each method executes in a new thread created
for it on the object's site.
This is reasonable
since threads in Mozart are extremely lightweight 
(millions can be created on one machine).

<ITEM> <EM>Sequential asynchronous stationary object</EM>:
In this object,
each method invocation uses one message only and does not
wait until the method is finished.
All method invocations execute in the same thread,
so the object is executed in a completely sequential way.
Non-caught exceptions in a method are ignored by the caller.
</LIST>
<P> Deciding between these three behaviors is done
when the object is created from its class.
A cached object is created with <<New>>,
a stationary object is created with <<NewStat>>,
and an sequential asynchronous stationary object
is created with <<NewSASO>>.
A stationary object is a good abstraction to
build servers--this is explained later in
the tutorial.
It is easy to program other distribution
behaviors in Oz.
We give examples of some of them
later in the tutorial.

<SUBSECTION><TITLE>Other stateful entities</TITLE>

<P> The other stateful language entities have the following
distribution behaviors:
<LIST>
<ITEM> <EM>Thread</EM>:
A thread actively executes a sequence of instructions.
The thread is stationary on the site it is created.
Threads communicate through shared data and
block when the data is unavailable, i.e.,
when trying to access unbound logic variables.
This makes Oz a dataflow language.
Threads are <EM>sited</EM> entities,
as is explained later in the tutorial.

<ITEM> <EM>Port</EM>:
A port is an asynchronous many-to-one channel
that respects FIFO for messages sent from within the same thread.
A port is stationary on the site it is created,
which is called its <EM>owner site</EM>.
The messages are appended to a stream
on the port's site.
Messages from the same thread appear
in the stream in the same order 
in which they were sent in the thread.

<P> Oz ports, which are a language concept,
should not be confused with Unix ports, which
are an OS concept.
Mozart applications will never use Unix ports explicitly
except when communicating with other applications
that have a Unix port interface.

<ITEM> <EM>Cell</EM>:
A cell is an updatable pointer to any other entity,
i.e., it is analogous to a standard updatable variable
in imperative languages such as C and Java.
Cells have the same distribution behavior
as cached objects.
Updating the pointer
may need up to three network messages,
but once the cell is local, then further
updates do not use the network any more.

<ITEM> <EM>Thread-reentrant lock</EM>:
A thread-reentrant lock allows only a single
thread to enter a given program region.
Locks can be created dynamically
and nested recursively.
Locks have the same distribution behavior
as cached objects and cells.
This implements a standard distributed mutual
exclusion algorithm.
</LIST>

<SUBSECTION><TITLE>Single-assignment entities</TITLE>

<P> An important category of language entities
are those that can be assigned only to one value:
<LIST>
<ITEM> <EM>Logic variable</EM>:
A logic variable resembles a single-assignment variable,
e.g., a <<final>> variable in Java.
It is more than that because two logic variables can
be bound together even before they are assigned,
and because a variable can be assigned more than
once, if it is always to the same value.
Logic variables are important for three reasons:
<LIST>
<ITEM> They have a more efficient protocol than cells.
Often, variables are just used like placeholders, that is,
they will be assigned only once.
It would be highly inefficient in a distributed system
to create a cell for that case.
<P> When a logic variable is bound,
the value is sent to its <EM>owner site</EM>,
namely the site on which it was created.
The owner site
then multicasts the value to all the
sites that have the variable.
The current release implements
the multicast as a sequence of message sends.
That is, if the variable is on <EM>n</EM> sites,
then a maximum of <EM>n</EM>+1 messages are
needed to bind the variable.
When a variable arrives on a site
for the first time, it is immediately
registered with the owner site.
This takes one message.
<ITEM> They can be used to improve latency tolerance.
A logic variable can be passed in a message
or stored in a data structure
before it is assigned a value.
When the value is there, then
it is sent to all sites that need it.
<ITEM> They are the basic mechanism
for synchronization and communication
in concurrent execution.
Mozart does not need an explicit
monitor or signal concept--rather,
logic variables let threads
wait for data availability,
which is 90% of the needs of concurrency,
and 9% more is provided by reentrant locking,
which is implemented by logic variables and cells.
The remaining 1% are not so simply
handled by these two cases and must be
programmed explicitly.
The reader is advised not to take
the above numbers too seriously.
</LIST>

<ITEM> <EM>Stream</EM>:
A stream is an asynchronous one-to-many communication channel.
In fact,
a stream is just a list whose last element is a logic variable.
If the stream is bound on the owner site,
then the binding is sent asynchronously to all sites that
have the variable.
Bindings from the same thread appear
in the stream in the same order 
that they occur in the thread.

<P>A port together with a stream implement an
asynchronous many-to-many channel
that respects the order of messages
sent from the same thread.
No order is enforced between
messages from different threads.

</LIST>

<SUBSECTION><TITLE>Stateless entities</TITLE>

<P> Stateless entities never
change, i.e., they do not have any internal state whatsoever.
Their distribution behavior is very efficient:
they are copied across the net in a single message.
The different kinds of stateless entities differ
in when the copy is done (eager or lazy)
and in how many copies of the entity
can exist on a site:
<LIST>

<ITEM> <EM>Records and numbers</EM>:
This includes lists and strings, which
are just particular kinds of records.
Records and numbers are copied eagerly across the network,
in the message that references them.
The same record and number may occur many
times on a site, once per copy
(remember that integers in Mozart
may have any number of digits).
Since these entities are so very basic
and primitive, it would be highly inefficient
to manage remote references to them
and to ensure that they exist only once on a site.
Of course, records and lists may refer
to any other kind of entity,
and the distribution behavior of that
entity depends on its type, not on
the fact of its being inside a record
or a list.

<ITEM> <EM>Procedures, functions, classes, functors,
atoms, and names</EM>:
These entities are copied eagerly across the
network, but can only exist once on a given site.
For example, an object class contains the code
of all the object's methods.
If many objects of a given class
exist on a site, then the class only exists once.

<P> It's important to distinguish the distribution
behaviors of objects and classes.
Classes are copied eagerly,
whereas objects are lazy.
I.e., if an object is passed to a site,
then the object's class is not copied to
the site until the object is called.
However, if a class is passed to a site,
then it is copied eagerly.

% <P> Computed functors, i.e., functors with
% external references, can be stateful.

<ITEM> <EM>Chunks</EM>:
A chunk is a lazy record.
If a chunk is passed to a site,
then the site initially only
has a reference to the chunk.
The chunk is copied only when a thread
on the site tries to access one of
the chunk's fields.
Chunks exist to make it easy to introduce
laziness into large data structures when needed.

</LIST>

<SUBSECTION><TITLE>Sited entities</TITLE>

<P> Entities that can be used only on one site
are called <EM>sited</EM>.
References to these entities can be passed to other
sites, but they do not work there.
They work only on their home site.

<P> In Mozart, a module is a record that
encapsulates related operations and
that possibly has some internal state.
The modules that are available in a Mozart
process when it starts up are called <EM>base modules</EM>.
The base modules contain all operations on
all basic Oz types.
There are additional modules, called <EM>system modules</EM>,
that are part of the system but loaded only when needed.
Furthermore, an application can define more
modules by means of functors that
importing from other modules.

<P> All base modules are unsited.
For example, a procedure that does additions
can be used on another site,
since the addition operation is 
part of the base module <<Number>>.
Due to limitations of the current release,
threads, dictionaries, arrays, and spaces are sited,
even though they are in base modules.
Many of these entities will become
unsited in future releases.
Some commonly-used base modules are
<<Number>>, <<Int>>, and <<Float>> (operations on numbers),
<<Record>> and <<List>> (operations on records and lists),
and
<<Procedure>>, <<Port>>, <<Cell>>, and <<Lock>>
(operations on common entities).

<P> We call <EM>resource</EM> any module
that is either a system module
or that imports directly or
indirectly from a system module.
All resources are sited.
The reason is that
they contain state, which is
either part of the emulator or
external to the Mozart process.
Access to this state 
is limited to the machine hosting
the Mozart process.
Some commonly-used system modules are
<<Tk>> and <<Browser>> (access to system graphics),
<<Connection>> and <<Remote>> (access to site-specific distributed operations),
<<Application>> and <<Module>> (access to application management),
<<Search>> and <<FD>> (access to constraint programming),
<<Open>> and <<Pickle>> (access to the file system),
<<OS>> and <<Property>> (access to the OS and emulator),
and so forth.

<SUBSECTION><TITLE>Bringing it all together</TITLE>

<P> Does the Mozart distribution model
give programmers a warm, fuzzy feeling when
writing distributed applications?
In short, yes it does.
The distribution model has been designed in tandem
with many application prototypes and incredible
numbers of Gedankenexperimenten.

<P> Here's how to use the distribution model 
to build applications.
The large-scale structure
of an application consists of a graph
of threads and objects, which access resources.
The objects are given distribution behaviors
to satisfy the geographic constraints
(placement of resources, dependencies between sites)
and the performance constraints
(network bandwidth, machine memory and speed).
Threads are created initially and during
execution to ensure that each site does
the desired part of the execution.
Objects exchange messages,
which may refer to objects
or other entities.
Records and procedures,
both stateless entities,
are the basic data structures
of the application--they are
passed between sites when needed.
Logic variables and locks are
used to manage the dataflow and the concurrency.

<P> Functors and resources are the key players
in distributed component-based programming.
A functor is essentially a component.
A functor is stateless, so it can be transparently copied
anywhere across the net and made persistent by pickling on a file
(see next section).
A functor is linked on a site by evaluating it there
with the site resources that it needs.
The result is a new resource, which can be
used to link more functors.
Our goal is for functors to be the core
technology driving an open community of developers,
who contribute to a growing global pool
of useful components.

<SECTION><TITLE>Global naming</TITLE>

<P> Global names are needed when an application
wants to interact with the world outside it.
Mozart supports three basic kinds of global names:
<LIST>
<ITEM> A <EM>ticket</EM> is a string that references a data
structure inside a running application.
Tickets are created by Mozart and
can be used to connect running applications together.
<ITEM> A <EM>URL</EM> is a string that references
a file across the network.
The string follows the standard URL syntax.
In Mozart the file can be a <EM>pickle</EM>, in which case
it can hold any kind of stateless data--procedures,
classes, functors, records, strings, etc.
<ITEM> A <EM>hostname</EM> is a string that
refers to a host (another machine) across the network.
The string follows the standard DNS syntax.
An application can use the hostname to start up another
Mozart process there.
</LIST>

<SUBSECTION><TITLE>Connecting applications by means of tickets</TITLE>

<P> Let's say Application 1 has a stream that
it wants others to access.
It can do this by creating a ticket
that references the stream.
The module <<Connection>> implements tickets.
Application 1 first creates a ticket for the stream
by using <<Connection>> as follows:

<<<
declare Stream Tkt in
{Connection.offer Stream Tkt}
{Show Tkt}
>>>

<P> Then Application 1 publishes the ticket somewhere so that
other applications can access it.
Our example displays
the ticket string in the emulator window.
We will use copy and paste to communicate the ticket
to another application.
The ticket looks something like
<<'x-ozticket://193.10.66.30:9002:SpGK0:U4v/y:s:f:xl'>>.
Don't worry about exactly what's inside the ticket.
Users don't normally see tickets:
they are stored in files or passed across the network,
e.g., in mail messages.
Application 2 can use the ticket to get a reference
to the stream:

<<<
declare Stream in
{Connection.take 
   'x-ozticket://193.10.66.30:9002:SpGK0:U4v/y:s:f:xl'
   Stream}
{Browse Stream}
>>>

<P> If Application 1 binds the stream
by doing <<Stream=a|b|c|_>>
then Application 2's browse window will show the bindings.

<P> The operation <<Connection.offer>> creates
a <EM>one-shot</EM> ticket, i.e., a ticket
that can be taken only once.
Within the module <<Connection>>,
a ticket that can be taken any
number of times can be defined
easily as follows:
<<<
declare
proc {Connection_many X ?Ticket ?Close}
   G={New Connection.gate init(X Ticket)}
in
   proc {Close} {G close} end
end
>>>
Given a reference <<X>>, <<Connection_many>>
defines a many-shot ticket <<Ticket>>
and a zero-argument procedure <<Close>>
that is used to close the ticket, i.e.,
to make the ticket invalid.
Attempting to take a ticket after a
close will raise an exception.


<SUBSECTION><TITLE>Persistent data structures</TITLE>

<P> An application can save
any stateless data structure
in a file and load it again from a file.
The loading may be done from a URL, used as 
a file's global name.
The module <<Pickle>> implements the saving
and loading, and the conversion to and from
the file format.

<P> For example, let's define a function and save it:

<<<
declare
fun {Fact N}
   if N=<1 then 1 else N*{Fact N-1} end
end

{Pickle.save Fact "~pvr/public_html/fact"}
>>>

<P> Since the function is in a <<public_html>> directory,
anyone can load it by giving a URL that specifies
the file:

<<<
declare
Fact={Pickle.load "http://www.sics.se/~pvr/fact"}

{Browse {Fact 10}}
>>>

<P> Anything stateless can be saved in a pickle,
including functions, procedures, classes,
functors, records, and atoms.
Stateful entities, such as objects and variables,
cannot be pickled.

<SUBSECTION><TITLE>Remote computations</TITLE>

<P> An application can start a computation
on a given host that uses the resources
of that host and
that continues to interact with the
application.
The computation is specified as a functor,
which is the standard way to
define computations with the resources they need.

<P> First we create a new Mozart process
that is ready to accept new computations:
<<<
declare
R={New Remote.manager init(host:"sinuhe.sics.se")}
>>>

<P> Let's make the process do some work.
We define a functor that does the work
when we evaluate it:
<<<
declare F M
F=functor export x:X define X={Fact 30} end 

M={R apply(url:'./' F $)}

{Browse M.x}
>>>

<P> The answer is returned to the client site in the module <<M>>,
which should not reference any resources.
Module <<M>> is a record; the answer is at <<M.x>>.
If it does, an exception will be raised in the thread
doing the <<apply>>.

<SUBSECTION><TITLE>Remote computations and functors</TITLE>

<P> A better solution than the one given
in the previous section is for the functor itself
to install the compute server on the remote site.
This completely <EM>separates</EM> the distribution aspect
(setting up the remote site to do the right thing)
from the computation(s) that we want to do.
We give this solution later in the tutorial.

<P> Another solution is to use a functor with
an external reference:
<<<
declare F M X in
F=functor define {Fact 30 X} end

M={R apply(url:'./' F $)}
{Browse X}
>>>

<P> This functor is not stateless,
but it's all right since we are not pickling the functor.
In fact, it's quite possible for
functors to have external references.
We discuss this later when we
talk about computed functors.

<SECTION><TITLE>Servers</TITLE>

<P> A server is a long-lived computation that provides a service to clients.
We will show progressively how to build different kinds of servers.

<SUBSECTION><TITLE>The hello server</TITLE>

<P> Let's build a basic server that returns the string <<"Hello world">>
to clients.
The first step is to create the server.
Let's do this and also make the server available through a URL:
<<<
% Create server
declare Str Prt Srv in
{NewPort Str Prt}
thread 
   {ForAll Str proc {$ S} S="Hello world" end}
end
proc {Srv X}
   {Send Prt X}
end

% Make server available through given URL:
{Pickle.save 
   {Connection_many Srv}
   "http://www.sics.se/~pvr/hw"}
>>>
<P>All the above must be executed on the server site.
Later on we will show how a client can create a
server remotely.

<P> Any client that knows the URL can access the server:
<<<
declare Srv in
Srv={Connection.take {Pickle.load "http://www.sics.se/~pvr/hw"}}

local X in
   {Srv X}
   {Browse X}
end
>>>
<P> This will show <<"Hello world">> in the browser window.

<P> What happens is that by taking the connection, the client
gets in its computation space a reference to the server.
This merges conceptually the client and server computation spaces
into a single computation space.
The client and server can then communicate as if
they were in the same process.
Later, when the client forgets the server reference,
the computation spaces become separate again.

<SUBSECTION><TITLE>Making stationary objects</TITLE>

<P> Stationary entities are a very important abstraction.
Mozart provides two primitive operations to
make entities stationary.
The first is creating a stationary object:
<<<
declare
Object={NewStat Class Init}
>>>
<P> When executed on a site,
the procedure <<NewStat>> takes a class and an initial message
and creates an object that is stationary on that site.
We define <<NewStat>> as follows:
<<<
declare
proc {NewStat Class Init Object}
   Object={MakeStat {New Class Init}}
end
>>>
<P> The procedure <<MakeStat>> takes an object or a one-argument
procedure and returns a one-argument procedure that
obeys exactly the same language semantics and is stationary.
We define <<{MakeStat PO StatP}>> as follows, where
<<PO>> is an object or a one-argument procedure
and <<StatP>> is a one-argument procedure:

<<<
declare
proc {MakeStat PO ?StatP}
   S P={NewPort S}
   N={NewName}
in
   proc {StatP M}
   R in
      {Send P M#R}
      if R==N then skip else raise R end end
   end
   thread
      {ForAll S
       proc {$ M#R}
          thread
             try {PO M} R=N catch X then R=X end
          end
       end}
   end
end
>>>

<P> <<StatP>> preserves exactly the same language semantics
as <<PO>>.  That is, it has the same concurrency
behavior and it raises the same exceptions.
The new name <<N>> is a globally-unique token.
This ensures that
there is no conflict with any exceptions
raised by <<ProcOrObj>>.


<NOTE FOOT>One-argument procedures are not exactly
objects, since they do not have features.
For all practical purposes, though, they
are identical.</NOTE>


<SUBSECTION><TITLE>The hello server with stationary objects</TITLE>

<P> The previous section showed how to build a basic server
using a port to collect messages.
There is in fact a much cleaner way,
namely by using stationary objects.
Here's how to create the server:
<<<
declare
class HelloWorld
   meth hw(X) X="Hello world" end
end

Srv={NewStat HelloWorld hw(_)} % Requires an initial method
>>>
<P> The client calls the server
by doing <<{Srv hw(X)}>>.
The class <<HelloWorld>> can be replaced by any class.
The only difference between this and creating
a centralized object is that <<New>> is replaced by <<NewStat>>.
This specifies the distributed behavior of the object.

<SUBSECTION><TITLE>A compute server</TITLE>

<P> One of the promises of distributed computing is making
computations go faster by exploiting the parallelism
inherent in networks of computers.
A first step is to create a compute server, that is,
a server that accepts any computation and uses
its computational resources to do the computation.
Here's one way to create a compute server:
<<<
declare
class ComputeServer
   meth init skip end
   meth run(P) {P} end
end

C={NewStat ComputeServer init}
>>>
<P> The compute server can be made available through a URL
as shown before.
Here's how a client uses the compute server:
<<<
declare
fun {Fibo N}
   if N<2 then 1 else {Fibo N-1}+{Fibo N-2} end
end

% Do first computation remotely
local F in
   {C run(proc {$} F={Fibo 30} end)}
   {Browse F}
end

% Do second computation locally
local F in
   F={Fibo 30}
   {Browse F}
end
>>>
<P> This first does the computation
remotely and then repeats it locally.
In the remote case, the variable <<F>>
is shared between the client and server.
When the server binds it, its value
is immediately sent to the server.
This is how the client gets a result from the server

<P> Any Oz expression <EM>E</EM> can be executed
remotely by prefixing it by
<<{C run(proc {$}>> and suffixing it by <<end)}>>.
Because Mozart is fully network-transparent,
<EM>E</EM> can be almost any expression
in the language: for example,
<EM>E</EM> can define new classes
inheriting from client classes.
The only expressions not allowed are
those using client resources.

<SUBSECTION><TITLE>A compute server with functors</TITLE>

<P> The solution of the previous section
is reasonable when the client and server
are independent computations that connect.
Let's now see how the
client itself can start up a compute
server on a remote site.
The client first creates a new Mozart process:
<<<
declare
R={New Remote.manager init(host:"sinuhe.sics.se")}
>>>

<P> Then the client sends a functor
to this process that, when evaluated,
creates a compute server:
<<<
declare F C
F=functor 
  export cs:CS
  define 
     class ComputeServer
        meth init skip end
        meth run(P) {P} end
     end
     CS={NewStat ComputeServer init}
  end

C={R apply(url:'./' F $)}.cs  % Set up the compute server
>>>

<P> The client can use the compute server as before:
<<<
local F in
   {C run(proc {$} F={Fibo 30} end)}
   {Browse F}
end
>>>


<SUBSECTION><TITLE>A dynamically-extensible server</TITLE>

<P> Sometimes a server has to be upgraded,
for example
to add extra functionality or to fix a bug.
We show how to upgrade a server
without stopping it.
This cannot be done in Java.
In practice, the upgrade can even
be done interactively.
A person sits down at a terminal anywhere
in the world, starts up an interactive Mozart
session, and upgrades the server while it is running.

<P> Let's first define a generic upgradable server:
<<<
declare
proc {NewUpgradableStat Class Init ?Upg ?Srv}
   Obj={New Class Init}
   C={NewCell Obj}
in
   Srv={MakeStat 
          proc {$ M} {{Access C} M} end}
   Upg={MakeStat
          proc {$ Class2#Init2} {Assign C {New Class2 Init2}} end}
end
>>>
<P> This definition must be executed 
on the server site.
It returns a server <<Srv>> and a stationary procedure
<<Upg>> used for upgrading the server.
The server is upgradable because it does
all object calls indirectly through the cell <<C>>.

<P> A client creates an upgradable compute server almost
exactly as it creates a fixed compute server,
by executing the following on the server site:
<<<
declare Srv Upg in
Srv={NewUpgradableStat ComputeServer init Upg}
>>>
<P> Let's now upgrade the compute server while
it is running.
We first define a new class <<CComputeServer>>
and then we upgrade the server with
an object of the new class:
<<<
declare
class CComputeServer from ComputeServer
   meth run(P Prio<=medium)
      thread
         {Thread.setThisPriority Prio}
         ComputeServer,run(P)
      end
   end
end

Srv2={Upg CComputeServer#init}
>>>

<P> That's all there is to it.
The upgraded compute server overrides
the <<run>> method with a new
method that has a default.
The new method supports the original call <<run(P)>>
and adds a new call <<run(P Prio)>>, where <<Prio>>
sets the priority of the thread doing computation <<P>>.

<P> The compute server can be
upgraded indefinitely
since garbage collection will remove
any unused old compute server code.
For example, it would be nice if the client could
find out how many active computations there
are on the compute server
before deciding whether or not to do
a computation there.
We leave it to the reader to upgrade
the server to add a new
method that returns the number of
active computations at each priority level.

</CHAPTER>

